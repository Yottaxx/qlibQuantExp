1. 核心设计哲学 (Design Philosophy)异构专家 (Heterogeneous Experts): 传统的 MoE 里的专家通常是同构的 FFN。在这里，我们将 Attention Heads 视为专家。Expert $\mathcal{T}$: 擅长捕捉时序动量/均值回归 (Time-Attention)。Expert $\mathcal{N}$: 擅长捕捉因子共线性/套利机会 (Cross-Sectional Attention)。状态感知门控 (State-Aware Gating): 绝对日期 (Absolute Date) 不参与底层的特征运算，而是作为 Router (路由) 的输入。Inductive Bias: 市场状态（Date）决定了当前是“动量主导”还是“选股主导”。各向异性编码 (Anisotropic Embedding): 必须严格区分“我是谁（Alpha ID）”、“我在哪（Relative Time）”和“现在是什么时候（Absolute Date）”。2. 架构拓扑 (Architecture Topology)输入张量：$X \in \mathbb{R}^{B \times T \times N}$。Phase 1: 3D-Encoding (三维坐标系建立)我们需要构建一个包含丰富语义的初始 Embedding $H_0$：$$H_0(b, t, n) = \text{Linear}(X_{b,t,n}) + \text{Emb}_{ID}(n) + \text{Emb}_{Pos}(t)$$Value Projection: 将原始因子值映射到 $d_{model}$。Factor ID Embedding ($N$-axis): 类似于 NLP 的 Word Embedding。让模型记住 Alpha_5 是“波动率类”，Alpha_12 是“动量类”。Relative Position Embedding ($T$-axis): 这里推荐使用 RoPE (Rotary Embedding) 作用于后续的 Time-Attention 层，或者使用 ALiBi。对于 $T=100$，简单的 Learnable Embedding 也够用。注意： 此处不加入 Date Embedding。Phase 2: Date-Gated MoE Block (堆叠 $L$ 层)这是核心模块。每一层都包含两个并行的 Attention 路径，通过 Date 进行动态加权。Step A: 生成路由权重 (The Router)输入：Batch 内的 Date Index。$$E_{date} = \text{DateEmbedding}(\text{Today})$$$$\text{GateWeights} = \text{Softmax}(\text{MLP}_{gate}(E_{date})) = [w_T, w_N]$$注意：$w_T + w_N = 1$。这是一个 Soft Routing 机制。Step B: 专家并行计算 (Parallel Experts)这里我们需要对张量进行 View 变换（Reshape/Permute）以利用标准的 Transformer 实现。Time Expert (Temporal Attention):Input View: $(B \times N, T, d_{model})$ —— 把每个因子的 100 天看作一条独立的句子。Operation: Causal Self-Attention (or Bidirectional with ALiBi).Why Causal? 即使是 Window，保持因果性有助于学习“事件推动”的逻辑。Output: $O_T \in (B \times N, T, d_{model}) \rightarrow \text{Reshape to } (B, T, N, d)$Factor Expert (Cross-Sectional Attention):Input View: $(B \times T, N, d_{model})$ —— 把每个时间步的 50 个因子看作一个 Sequence。Operation: Full Self-Attention (无 Mask)。让所有因子在当前时刻交互。Output: $O_N \in (B \times T, N, d_{model}) \rightarrow \text{Reshape to } (B, T, N, d)$Step C: 门控融合 (Gated Fusion)$$H_{mid} = w_T \cdot O_T + w_N \cdot O_N$$$$H_{out} = \text{LayerNorm}(H_{in} + \text{Dropout}(H_{mid}))$$Step D: FFN (Point-wise)$$H_{next} = \text{LayerNorm}(H_{out} + \text{FFN}(H_{out}))$$3. 为什么这个架构优越？(The "Why")信息交互 (Interaction): 虽然在单层内 $T$ 和 $N$ 是并行计算的，但通过堆叠多层（Stacking Layers），信息的 Receptive Field (感受野) 会呈螺旋上升。Layer 1: Alpha 1 知道了自己 T-1 的信息。Layer 2: Alpha 1 知道了 Alpha 2 在 Layer 1 处理过的信息（其中包含了 Alpha 2 的 T-1 信息）。结果：Layer $L$ 实现了全时空交互。非平稳性建模 (Non-stationarity): Date Embedding 作为一个 Global Prior，它不污染底层的特征空间，而是作为 Meta-Learner 指挥模型调整注意力偏好。这完美符合量化中 Regime Shift 的直觉。计算效率: $T=100, N=50$。Time Attn: $B \times 50 \times 100^2$Factor Attn: $B \times 100 \times 50^2$这比标准的 Flatten Attention ($B \times (50\times100)^2$) 要小几个数量级。